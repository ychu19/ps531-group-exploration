---
title: "| Exploration 1---Statistical Inference  \n| The problem of 'controlling for'
  for\n| Causal inference and Description\n"
author: "Jake Bowers"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  pdf_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    keep_tex: yes
    latex_engine: xelatex
  html_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4
geometry: margin=1in
graphics: yes
header-includes:
- \usepackage[T1]{fontenc}
- \usepackage{textcomp}
- \usepackage{fontspec}
- \newfontfamily\unicodefont[Ligatures=TeX]{TeX Gyre Heros}
- \newfontfamily\themainfont[Ligatures=TeX]{Crimson Text}
- \newfontfamily\grouptwofont[Ligatures=TeX]{Source Code Pro}
- \newfontfamily\groupthreefont[Ligatures=TeX]{Courier}
mainfont: Crimson Text
fontsize: 10pt
bibliography: classbib.bib
---


<!-- Make this document using library(rmarkdown); render("exploration1.Rmd") -->
\input{mytexsymbols}


```{r include=FALSE, cache=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.

## To make the html file do
## render("exploration1.Rmd",output_format=html_document(fig_retina=FALSE))
## To make the pdf file do
## render("exploration1.Rmd",output_format=pdf_document())

require(knitr)
opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small",    # slightly smaller font for code
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='footnotesize',
  fig.retina=FALSE,
  message=FALSE,
  comment=NA)
```


```{r initialize, echo=FALSE}
##First, just setup the R environment for today:
if(!file.exists('figs')) dir.create('figs')

options(SweaveHooks=list(fig=function(){
			   par(mar=c(3.5, 3, 1.1, 0),
			       pty="s",
			       mgp=c(1.5,0.5,0),
			       oma=c(0,0,0,0))},
			 echo=function(){options(continue=" ") ##Don't show "+" prompts,
			 options(prompt=" ")
			 }),
	digits=4,
	scipen=8,
	width=132
	)
options(error=function(){options(prompt="> ",continue="+ ");NULL})
```


Your friend at the UN (who needs a name) was so impressed with your help, and
especially with the videos of the Quincunx demonstrating the relationship
between the binomial and the Normal distribution, that she has called back.
She says, "After I presented the news that the bombing caused an increase in
volunteering of about 45 minutes, I was immediately attacked by people
claiming that I was making unfair comparisons. Maybe, they said, the people
interviewed before the bombing were just less active in general for all sorts
of other reasons having nothing to do with the bombing, but, after the
bombing, the survey field research team tended to interview people who were
likely to be active --- like people with more education, etc. I hired another
couple of analysts to look into this --- to make my comparisons more fair and
I'm wondering if you'd take a look at what they did before they fled...mean,
left and went to bed."

#In this paragraph, my friend concerns the potential danger of sampling bias; according to her co-workers, the people who were interviewed post bombing can be more socially active and received more education than the people who were interviewed before bombing.
Then, what would be the potential danger of sampling bias in this survey? Perhaps, one of the possible problems is that sampling bias might lead to misleading results and interpretation of the survey results.
One of the most important prerequisite for observational study is keeping the randomness of the sample. Without properly ensuring randomness of the sample, results of the study might not accurately measure the treatment effects of the observational study. If the people who were interviewed after bombing is more socially active and received more education compare to the people who were interviewed before bombing, the results of the survey might overestimate the treatment effect (in this case, bombing) to the dependent variable (the number of hours people spent in helping others).  


\begingroup
\unicodefont
Memo: To UN CVE Director,

```{r tidy=FALSE}
load(url("http://jakebowers.org/Data/ho05.rda"))
wrkdat<-ho05[!is.na(ho05$hlphrs)&ho05$Rage!=0,] ## removing bad obs 
 ## There is one Caribbean lady did not report her age in the survey.
effect1<-coef(lm(hlphrs~postbomb,data=wrkdat))[["postbomb"]] 
 ## shows the coefficient only
effect1

covariatesLabels <- c( "GOR" = "Government Office Region",
		      "Rsex" = "Gender",
		      "Rdmstat" = "Respondent de facto marital status",
		      "Rage" = "Age",
		      "Ethnic11" = "Ethnicity",
		      "RILO4A" = "Economic Status",
		      "hhinc" = "Household Income",
		      "ZQuals1" = "Highest qualification: includes 70+ (???)",
		      "DVHSize" = "Number of people in household",
		      "immigrant" = "Immigrant",
		      "workstatus" = "Employment status",
		      "SLive" = "Years lived in neighborhood",
		      "relig.and.act" = "Interaction of religion and practicing questions",
		      "Rnssec17" = "NSSec grouped into 17 categories (???)",
		      "HTen1" = "Owns or rents",
		      "Rage5cat" = "5 level categorical variable for age",
		      "hhinc5cat" = "5 level cateogrical variable for household income",
		      "DVHSizeCat" = "Categorical coding of household size: 1, 2, 3, or 4+" ,
		      "SLive5cat" = "5 level cateogrical variable for years lived in neighborhood")

covariates <- names(covariatesLabels)
## Changing the column labels into meaningful names
```

This summary of the effect of the bombings has a couple of problems: First, we
wondered about whether summarizing differences using a simple mean might be
misleading --- we might not use the mean as a one-number summary in the first
place, but might prefer something else that is less sensitive to extreme
values (like a median-regression or some robust/resistant regression using an
M-estimator). 

#In this sentence, the author asks whether summarizing differences between two groups using simple mean would be appropriate or misleading. I think the idea behind this question is closely related to the appropriateness of using t-test in comparing two groups.      
T –test confirms whether means of two groups are significantly different from each other. Thus, t-test compares simple mean (not median) between two independently sampled groups and check whether the means of two groups are statistically different from each other. 
T – test assumes two conditions. It assumes normal distribution and equal group variance (in other words, homogeneity between the two groups). (Fox, 2016). 
In the previous section, the author expresses her concern about the heterogeneity between the two sampled groups; the people who were interviewed post bombing can be more socially active and received more education than the people who were interviewed before bombing. Thus, based on this concern, comparing the two groups using simple mean can be problematic as the author suggests.  

Second, and most importantly, after reading
@rosenbaum2010design (especially chapters 1, 3, 7, 8, 9, 13) and
@gelman2007dau (especially chapters 9 and 10) we wondered whether we
could claim that we were comparing like with like --- that, in fact, the
comparison of behavior before versus after the bombing could be said to be
like a natural experiment allowing a clear counterfactual
comparison.\footnote{We also recommend @gerber2012field Chap 1 and 2 for more on
  counterfactual comparisons if the issue is not clear from Rosenbaum or
  Gelman and Hill.}

#It is very important to make sure that we are comparing like with like when 
trying to argue for an average treatment effect on our dependent variable. 
To understand the effect of an explanatory variable on an dependent variable, 
a researcher can imagine an hypothetically counterfactual universe where everything 
is the same with the real world but the explanatory variable, and compare the 
differences of the dependent variable between the reality and the hypothetical 
counterfactual universe. An ideally perfect experiment should have everything 
exactly the same between the control and treatment groups except the treatment, 
so that researchers can observe the differences of outcomes from the two groups 
and attribute the differences solely to the treatment. In real experiments, 
since there may always be problems of unit homogeneity and omitted variable bias, 
which may lead to treatment effect heterogeneity, researchers usually consider 
randomized treatment assignment a tool to ensure that different treatment groups are `balanced', 
so that the control and treatment groups are comparable.

#However, in observational studies where treatments are observed rather than assigned, 
like what our friend from UN wants to know about the effect of bombing on people’s 
civic activities, it is not always reasonable to consider the observed data as random 
samples from a common population (Gelman and Hill 2007, p. 181). There are several ways 
to make sure that we are `comparing like with like’ in observational studies, and we will 
talk more about what are the possible approaches to `balance’ the control (i.e. pre-bombing 
in our case) and treatment (i.e. post-bombing) groups.

These plots suggest do not make us worry too much about the common
support/extrapolation problem with respect to two covariates (assuming that
residential tenure and household income are truly covariates and not
post-intervention variables). However, these plots do raise questions about
the parallel lines requirement.

```{r }
effect2lm<-lm(hlphrs~postbomb+SLive,data=wrkdat)
effect3lm<-lm(hlphrs~postbomb+hhinc,data=wrkdat)
```

```{r, echo=TRUE,eval=TRUE,fig.show='asis',out.width=".95\\textwidth",fig.width=12,fig.height=6}
par(mfrow=c(1,2),pty="s",mgp=c(1.5,.5,0),oma=rep(0,4))

with(wrkdat,{
       plot(SLive,hlphrs,
	    pch=c(1,19)[postbomb+1],
	    cex=c(1.3,.8)[postbomb+1],
	    col=c("black","gray")[postbomb+1])
       abline(coef(effect2lm)[1]+coef(effect2lm)[2]*1,coef(effect2lm)[3],col="gray",lwd=2)
       abline(coef(effect2lm)[1]+coef(effect2lm)[2]*0,coef(effect2lm)[3],lwd=2)
       plot(hhinc,hlphrs,
	    pch=c(1,19)[postbomb+1],
	    cex=c(1.3,.8)[postbomb+1],
	    col=c("black","gray")[postbomb+1])
       abline(coef(effect3lm)[1]+coef(effect3lm)[2]*1,coef(effect3lm)[3],col="gray",lwd=2)
       abline(coef(effect3lm)[1]+coef(effect3lm)[2]*0,coef(effect3lm)[3],lwd=2)
}
)
legend(x="topright",pch=c(1,19),
       col=c("black","gray"),
       legend=c("PreBomb","PostBomb"))
```

We next tried to control for both variables, but this next plot caused us to
worry about the curse of dimensionality. 
#In this paragraph, the author expresses her concern about the curse of dimensionality which is an obstacle a nonparametric regression. The curse of dimensionality refers to the phenomenon that the rapidly declining number of points near a focal point derives from the increasing number of explanatory variables (Fox, 2016). Non-parametric regression constitutes a regression line (which consists of local polynomial regression) using the average values of observations. Thus, in this case, the number of observations are closely related to the explanatory power of the variables; the bigger the number of observations, the explanatory power of the regression line generated from the explanatory variables is likely to be stronger. 
In this context, dimension refers to the number of variables. If the number of variables increases while the number of observations is consistent, since the dimensions increase, the number of observations that can be used to explain phenomena around certain space reduces. Thus, increasing the dimensions near certain focal points might hinder accurate estimation of the effects of explanatory variables because estimation based on small number of variables are more prone to the biased estimate of the effects (Fox, 2016).    


For example, you cannot claim to
control for both high household income and long home tenure when making this
comparison: years lived in a neighborhood are related to household income and
so the data are not spread evenly across the scatterplot.

```{r }
effect4lm<-lm(hlphrs~postbomb+hhinc+SLive,data=wrkdat)
```

```{r out.width=".5\\textwidth", fig.align="center"}
with(wrkdat,{ plot(hhinc,SLive,
		   pch=c(1,19)[postbomb+1],
		   cex=c(1.3,.8)[postbomb+1],
		   col=c("black","gray")[postbomb+1]) })

```
\endgroup


\themainfont

Your friend asks, "What are they talking about? Why should the first plots
make me feel good but the second plot not? Why can't I claim that I've
controlled for two variables in some overarching manner? What is the problem?"

Then she continues, "Because I didn't like the answers I was getting from
those guys, I fired them and hired someone else who claimed that it would be
easy to control for as many variables as a I wanted. Here is what he said."

\begingroup
\groupthreefont
This is easy. This is exactly what multiple regression was built for. Watch this!

```{r results="hide"}

lm1<-lm(hlphrs~GOR + Rsex + Rdmstat + Rage + Ethnic11 + RILO4A + hhinc +
	ZQuals1 + DVHSize + immigrant + workstatus + SLive + relig.and.act +
	Rnssec17 + HTen1, data=wrkdat)

hlphrsWithNoCovs<-residuals(lm1)

lm2<-lm(postbomb~GOR + Rsex + Rdmstat + Rage + Ethnic11 + RILO4A + hhinc +
	ZQuals1 + DVHSize + immigrant + workstatus + SLive + relig.and.act +
	Rnssec17 + HTen1, data=wrkdat)

postbombWithNoCovs<-residuals(lm2)

effect5lm<-lm(hlphrsWithNoCovs~postbombWithNoCovs)
coef(effect5lm)[[2]]

lmbig<-lm(hlphrs~postbomb+GOR + Rsex + Rdmstat + Rage + Ethnic11 + RILO4A + hhinc +
	ZQuals1 + DVHSize + immigrant + workstatus + SLive + relig.and.act +
	Rnssec17 + HTen1, data=wrkdat)

```
\endgroup

"But,", your friend sighs, "Now I don't get this. What is this person doing?
In what sense does the `coef(effect5lm)[[2]]` represent a comparison
that has been purged of possible confounding? Has this approach 'controlled
for' enough?^[To answer the first part, you might consider the more
  traditional multiple regression way of controlling for many variables.]

The meaning of `control' for in theory and in this analysis. What are the differences between controling for one variable, two variables and all the other variables? 

After you answer her questions, she calls back. "I fired this second analyst
and hired another group. They claim that they can help me know whether I am
adjusting or controlling for background, observed, covariates appropriately or
not. Can you help me judge whether they are making sense?"

\begingroup
\grouptwofont
Dear Commander,

Here is another approach which is based on the idea that we should be able to
report on how good of a job we did at controlling for. That is, statistical
adjustment should have standards.

First, we might ask whether the initial comparison made sense: were people
interviewed after the bombing that different from people interviewed before
the bombing? This next approach was developed by @hansen2008cbs.

```{r results='hide'}
library(RItools)

balfmla<-reformulate(covariates[1:15],response="postbomb")

xb1<-xBalance(balfmla,data=wrkdat,
	      report=c("std.diffs","z.scores","adj.means",
                    "adj.mean.diffs", "chisquare.test","p.values"))

print(xb1,horizontal=TRUE,show.pvals=TRUE)
## xb1$overall
## xb1$results

```

For example, we can say that none of the variables had pre-vs-post-bombing
differences larger, in absolute value, than about .23 of a standard deviation
and that the worst differences pre-vs-post bombing were in terms of whether
people lived in the North West region, White
ethnicity, and martial status. For example, the sample before the bombing was
about 68\% White ethnicity but about 56\% White ethnicity after the bombing.
Overall, however, we do not have enough information to argue strongly against
the null hypothesis that people interviewed before and after the bombing
differed only as they would have differed in a randomized experiment. Notice,
for example, about 9\% of the people before the bombing refused to report a
household income (hhinc==NA) and about 11\% of people interviewed after the
bombing refused to report a household income.

```{r results="hide"}
summary(abs(xb1$results[,"std.diff","unstrat"]))
xb1$results[order(abs(xb1$results[,"std.diff","unstrat"])),"std.diff","unstrat"]
xb1$results["Ethnic11White",,]
xb1$results[c("hhinc","hhinc.NATRUE"),,]
xb1$overall
```

Our research group next tried to improve this "balance". We would like to
create sets which only compare at least one person interviewed before the
bombing with at least one person after the bombing. These sets are supposed to
contain people who are similar in terms of all of the background covariates
--- so that our pre-vs-post comparisons ought not reflect differences due to
the background covariates (and thus, reflect either differences due to the
bombing or to covariates that we do not observe). We have not yet been paid by
your organization so we leave the interpretation of the following to you.

```{r }
library(optmatch)

newdata<-fill.NAs(balfmla,data=wrkdat) ## We want to match on missingness too
names(newdata)<-make.names(names(newdata))

propensityModel<-glm(postbomb~.,data=newdata,family=binomial(link="logit"))
psDistMat<-match_on(propensityModel,data=newdata)

mhDistMat<-match_on(postbomb~.,data=newdata,method="rank_mahalanobis")
summary(as.vector(psDistMat))

##tmp <- wrkdat$Rage
##names(tmp) <- rownames(wrkdat)
##absdist <- match_on(tmp, z = wrkdat$postbomb,
##                  within = exactMatch(postbomb ~ immigrant, wrkdat))


## Following Rosenbaum Chapter 8: mahalanobis distance with propensity caliper
## fm1<-fullmatch(mhDistMat+caliper(psDistMat,1)+caliper(absdist,10),data=newdata)
fm1<-fullmatch(mhDistMat+caliper(psDistMat,1),data=newdata)

summary(fm1,max.controls=Inf,propensity.model=propensityModel)
table(matched(fm1))

xb2<-xBalance(postbomb~.,strata=list(unstrat=NULL,fm1=~fm1),
	      data=newdata,
	       report=c("std.diffs","z.scores","adj.means",
                    "adj.mean.diffs", "chisquare.test","p.values"))


wrkdat$fm1<-as.factor(fm1)
effect6lm<-lm(hlphrs~postbomb+fm1,data=wrkdat)
coef(effect6lm)[[2]]

xBalance(postbomb~hlphrs,strata=list(unstrt=NULL,fm1=~fm1),
	 data=wrkdat,report=c("std.diffs","z.scores","adj.means",
                    "adj.mean.diffs", "chisquare.test","p.values"))

```

\endgroup

\themainfont

Your friend then asks for help. Which estimate of the effect of the bombing
should she use in her report? Why? Is there another summary of the pre-vs-post
bombing behavior that she should be using?


```{r }
save(fm1,file="fm1.rda")
```

# References

Fox, John. Applied Regression Analysis and Generalized Linear Models. Los Angeles: Sage, 2016.
